{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBKoCvmf7ANr"
   },
   "source": [
    "# Installing Relevant Libraries To Store the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GH18Ay2mZ3mx",
    "outputId": "cb60e4d0-f37c-4850-dc08-c0b33572c4ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.5/421.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.0/284.0 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.0/49.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "lida 0.0.10 requires fastapi, which is not installed.\n",
      "lida 0.0.10 requires kaleido, which is not installed.\n",
      "lida 0.0.10 requires uvicorn, which is not installed.\n",
      "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mCollecting detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6\n",
      "  Cloning https://github.com/facebookresearch/detectron2.git (to revision v0.6) to /tmp/pip-install-tyqbi5jv/detectron2_c6f1545156fc4b91bbfaa5883ba1d6bc\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-install-tyqbi5jv/detectron2_c6f1545156fc4b91bbfaa5883ba1d6bc\n",
      "  Running command git checkout -q d1e04565d3bec8719335b88be9e9b961bf3ec464\n",
      "  Resolved https://github.com/facebookresearch/detectron2.git to commit d1e04565d3bec8719335b88be9e9b961bf3ec464\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (from detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (10.2.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (3.7.1)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (2.0.7)\n",
      "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (2.4.0)\n",
      "Collecting yacs>=0.1.8 (from detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6)\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (0.9.0)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (2.2.1)\n",
      "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.10/dist-packages (from detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (4.66.1)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (2.15.1)\n",
      "Collecting fvcore<0.1.6,>=0.1.5 (from detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6)\n",
      "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting iopath<0.1.10,>=0.1.7 (from detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6)\n",
      "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (0.18.3)\n",
      "Requirement already satisfied: pydot in /usr/local/lib/python3.10/dist-packages (from detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (1.4.2)\n",
      "Requirement already satisfied: omegaconf>=2.1 in /usr/local/lib/python3.10/dist-packages (from detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (2.3.0)\n",
      "Collecting hydra-core>=1.1 (from detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6)\n",
      "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting black==21.4b2 (from detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6)\n",
      "  Downloading black-21.4b2-py3-none-any.whl (130 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.0/131.0 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from black==21.4b2->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (8.1.7)\n",
      "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from black==21.4b2->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (1.4.4)\n",
      "Requirement already satisfied: toml>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from black==21.4b2->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (0.10.2)\n",
      "Requirement already satisfied: regex>=2020.1.8 in /usr/local/lib/python3.10/dist-packages (from black==21.4b2->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (2023.12.25)\n",
      "Collecting pathspec<1,>=0.8.1 (from black==21.4b2->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6)\n",
      "  Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from black==21.4b2->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (1.0.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (1.23.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (6.0.1)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from hydra-core>=1.1->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (4.9.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from hydra-core>=1.1->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (23.2)\n",
      "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath<0.1.10,>=0.1.7->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (2.8.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (1.60.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (2.17.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (3.5.2)\n",
      "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (2.31.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (67.7.2)\n",
      "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (2.1.4)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6) (3.2.2)\n",
      "Building wheels for collected packages: detectron2, fvcore\n",
      "  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for detectron2: filename=detectron2-0.6-cp310-cp310-linux_x86_64.whl size=5726049 sha256=e6ceb3107f5761555e5ecde413397e10ad40fbee916407ca40ae880b08a1d01e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-xfequyii/wheels/d0/62/a4/633b274d7706eff61ae574ae90fca694eeb99efbc2d719069f\n",
      "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=20de688aeb1d9418687db7dbff8cbd268ce59ea1c4d952369d64796106ad0513\n",
      "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
      "Successfully built detectron2 fvcore\n",
      "Installing collected packages: yacs, pathspec, iopath, hydra-core, fvcore, black, detectron2\n",
      "  Attempting uninstall: iopath\n",
      "    Found existing installation: iopath 0.1.10\n",
      "    Uninstalling iopath-0.1.10:\n",
      "      Successfully uninstalled iopath-0.1.10\n",
      "Successfully installed black-21.4b2 detectron2-0.6 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 pathspec-0.12.1 yacs-0.1.8\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  poppler-utils\n",
      "0 upgraded, 1 newly installed, 0 to remove and 32 not upgraded.\n",
      "Need to get 186 kB of archives.\n",
      "After this operation, 696 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.3 [186 kB]\n",
      "Fetched 186 kB in 1s (128 kB/s)\n",
      "Selecting previously unselected package poppler-utils.\n",
      "(Reading database ... 121730 files and directories currently installed.)\n",
      "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.3_amd64.deb ...\n",
      "Unpacking poppler-utils (22.02.0-2ubuntu0.3) ...\n",
      "Setting up poppler-utils (22.02.0-2ubuntu0.3) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      "Collecting pinecone-client\n",
      "  Downloading pinecone_client-3.0.2-py3-none-any.whl (201 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2023.11.17)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.9.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.0.7)\n",
      "Installing collected packages: pinecone-client\n",
      "Successfully installed pinecone-client-3.0.2\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n",
      "Installing collected packages: tiktoken\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llmx 0.0.15a0 requires cohere, which is not installed.\n",
      "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed tiktoken-0.5.2\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (10.2.0)\n",
      "Collecting openai==0.28.0\n",
      "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28.0) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28.0) (4.66.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28.0) (3.9.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.0) (2023.11.17)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (4.0.3)\n",
      "Installing collected packages: openai\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed openai-0.28.0\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.1.5-py3-none-any.whl (806 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.7/806.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.24)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.4)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.17 (from langchain)\n",
      "  Downloading langchain_community-0.0.17-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langchain-core<0.2,>=0.1.16 (from langchain)\n",
      "  Downloading langchain_core-0.1.18-py3-none-any.whl (237 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.0/237.0 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langsmith<0.1,>=0.0.83 (from langchain)\n",
      "  Downloading langsmith-0.0.86-py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.14)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
      "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain) (3.7.1)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain) (1.2.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Installing collected packages: jsonpointer, langsmith, jsonpatch, langchain-core, langchain-community, langchain\n",
      "Successfully installed jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.5 langchain-community-0.0.17 langchain-core-0.1.18 langsmith-0.0.86\n"
     ]
    }
   ],
   "source": [
    "!pip install unstructured -q\n",
    "!pip install unstructured[local-inference] -q\n",
    "!pip install detectron2@git+https://github.com/facebookresearch/detectron2.git@v0.6   #egg=detectron2 -q\n",
    "!apt-get install poppler-utils\n",
    "!pip install pinecone-client\n",
    "!pip install tiktoken\n",
    "#!pip install pillow==8.3.2\n",
    "!pip install pillow\n",
    "!pip install openai==0.28.0\n",
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TMIJuBN-L_v"
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t04vuUcgnK44"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "import pandas as pd\n",
    "from pinecone import Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7zKtpku934a"
   },
   "source": [
    "# Directory Loader Function Take the Data From Directory to Store Data There"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EG9uBd8MnRKG",
    "outputId": "74b53716-9ed4-4a88-99d6-6f2840b99178"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the directory where the PDF documents are stored: /content/drive/MyDrive/content\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "#### directory = ''\n",
    "'\n",
    "'''\n",
    "The purpose of this function is to store data\n",
    "'''\n",
    "def load_docs():\n",
    "    # Get user input for the directory\n",
    "    directory = input(\"Enter the directory where the PDF documents are stored: \")\n",
    "    # Use the provided directory in the loader\n",
    "    loader = DirectoryLoader(directory, glob='**/*.pdf')\n",
    "    #Loader.Load Function Load the the in pdf_documents Variable\n",
    "    pdf_documents = loader.load()\n",
    "    # Return Pdf Files\n",
    "    return pdf_documents\n",
    "\n",
    "#This Functions is taking Pdf_docs\n",
    "pdf_documents = load_docs()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N_5Y82_y3hM"
   },
   "source": [
    "#### Printing How Many Documents Are There"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kwHVV2kotupz",
    "outputId": "14df4988-ce9a-48ad-f946-3af9a81e2ce7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking How Many Number Of Documents Are There 19\n"
     ]
    }
   ],
   "source": [
    "#Checking the Numeber of Documents\n",
    "print('Checking How Many Number Of Documents Are There', len(pdf_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvLcal0wzDKJ"
   },
   "source": [
    "# Splitting Docs Into Chunks Due to Token Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ahYOqOX3ufpb",
    "outputId": "0dfc743b-dbb0-4941-df8a-cc566d9e4ba4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Number of Chunks Are 1321\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The Purpose Of This Function is to Split Docs.\n",
    "\n",
    "'''\n",
    "def split_docs(documents,chunk_size=1000,chunk_overlap=20):\n",
    "  #Text Splitter Function Split the Data Into Chunks\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "  #Splitting Documents Using Split Documents Function\n",
    "  docs = text_splitter.split_documents(documents)\n",
    "  #Return Docs\n",
    "  return docs\n",
    "#Calling Split_Docs Function to Get Chunks\n",
    "Chunks = split_docs(pdf_documents)\n",
    "#Printing the total Number of Chunks\n",
    "print('The Total Number of Chunks Are',len(Chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91VPQF3WBZwv"
   },
   "source": [
    "# Intialize Embeddings Through Openai Pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uy5T_G87aMH-",
    "outputId": "e3aaf3bc-a7f2-45d8-fabc-f9396302846c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# get openai api key from platform.openai.com\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') or ''\n",
    "\n",
    "def get_openai_transformer_embeddings(model_name=\"text-embedding-3-large\"):\n",
    "  return OpenAIEmbeddings(model=model_name,openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "embeddings_model = get_openai_transformer_embeddings(model_name=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAuzxDTGBSii"
   },
   "source": [
    "#Now We Will Get Chunks and Pdf Names To Store As a Metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9Dc4BAu6jDv",
    "outputId": "21a2e480-c486-4b97-e0a3-989b807a36cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Number of Chunks Are 1321\n",
      "PDF Names Are: 1321\n"
     ]
    }
   ],
   "source": [
    "def extract_chunks_and_pdf_names(chunks):\n",
    "    list_of_chunks = [chunk.page_content for chunk in chunks]\n",
    "    pdf_names = [chunk.metadata['source'] for chunk in chunks]\n",
    "    return list_of_chunks, pdf_names\n",
    "\n",
    "# Example usage:\n",
    "list_of_Chunks, Pdf_Name = extract_chunks_and_pdf_names(Chunks)\n",
    "\n",
    "# Printing the total number of chunks and corresponding PDF names\n",
    "print('The Total Number of Chunks Are', len(list_of_Chunks))\n",
    "print('PDF Names Are:', len(Pdf_Name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqNcWbqrDb7v"
   },
   "source": [
    "# Generate Embeddings And Store Into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5hsLXF9sb-O_",
    "outputId": "6efd3641-7199-44bc-9732-3a9b93949bfc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_community.embeddings.openai:Warning: model not found. Using cl100k_base encoding.\n"
     ]
    }
   ],
   "source": [
    "def create_embedding_dataframe(embeddings_model, list_of_Chunks, Pdf_Name):\n",
    "    # Generate embeddings for each chunk\n",
    "    embeddings = embeddings_model.embed_documents(list_of_Chunks)\n",
    "    # Generate unique IDs for each chunk\n",
    "    chunk_ids = [f'vec{i+1}' for i in range(len(list_of_Chunks))]\n",
    "    # Format chunks with metadata\n",
    "    formatted_chunks = [{\"text\": chunk, \"Source\": pdf_file_name} for chunk, pdf_file_name in zip(list_of_Chunks, Pdf_Name)]\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'id': chunk_ids,\n",
    "        'values': embeddings,\n",
    "        'metadata': formatted_chunks\n",
    "    })\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "# Assuming embeddings_model, list_of_Chunks, and Pdf_Name are already defined\n",
    "df = create_embedding_dataframe(embeddings_model, list_of_Chunks, Pdf_Name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYDmsnRlD3vx"
   },
   "source": [
    "#### Generate Vector to Store Data In Pinecone Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e1tKQMrucXbI"
   },
   "outputs": [],
   "source": [
    "def create_vectors_from_dataframe(df):\n",
    "    # Initialize an empty list to store the vectors\n",
    "    vectors = []\n",
    "    # Iterate through DataFrame rows\n",
    "    for _, row in df.iterrows():\n",
    "        vector = {\n",
    "            \"id\": row['id'],\n",
    "            \"values\": row['values'],\n",
    "            \"metadata\": {\"text\": row['metadata']['text'], \"source\": row['metadata']['Source']}\n",
    "        }\n",
    "        vectors.append(vector)\n",
    "    return vectors\n",
    "\n",
    "result_vectors = create_vectors_from_dataframe(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtzqCA_HE_H-"
   },
   "source": [
    "# Intialize Pinecone Database and Fetch index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_EG9nrdgLsD"
   },
   "outputs": [],
   "source": [
    "def initialize_pinecone(api_key=\"\", environment=\"\", index_name=\"\"):\n",
    "  pc = Pinecone(api_key=api_key, environment=environment)\n",
    "  index = pc.Index(index_name)\n",
    "  return index\n",
    "\n",
    "pinecone_index = initialize_pinecone()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-sIt2IKFbr8"
   },
   "source": [
    "# Store Vector Data In terms of Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v-XOHWY_h9SS"
   },
   "outputs": [],
   "source": [
    "def upsert_vectors_in_batches(index, vectors, vector_length=1000, batch_size=30, namespace=\"ns1\"):\n",
    "    if len(vectors) < vector_length:\n",
    "        # If the list is smaller than the batch size, upsert in a single batch\n",
    "        index.upsert(vectors, namespace=namespace)\n",
    "    else:\n",
    "        for batch_start in range(0, len(vectors), batch_size):\n",
    "            batch_end = min(batch_start + batch_size, len(vectors))\n",
    "            batch_data = vectors[batch_start:batch_end]\n",
    "            # Use Pinecone's batch upsert function to insert or update documents in batch\n",
    "            index.upsert(batch_data, namespace=namespace)\n",
    "\n",
    "# Assuming 'pinecone_index' is already defined and 'result_vectors' is the list of vectors\n",
    "upsert_vectors_in_batches(pinecone_index, result_vectors, batch_size=30, namespace=\"ns1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9W6R9g4mHQ-Q"
   },
   "source": [
    "#Get Similar Docs From Vector Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5zkoHngfqtXF"
   },
   "outputs": [],
   "source": [
    "def get_similiar_docs(index,query, k=5, score=False):\n",
    "    result=index.query(\n",
    "    namespace=\"ns1\",\n",
    "    vector=embeddings_model.embed_query(query),\n",
    "    top_k=k,\n",
    "    include_values=True,\n",
    "    include_metadata=True)\n",
    "    return result['matches'][0]['metadata']['text']+\"\\n\"+result['matches'][1]['metadata']['text']+\"\\n\"+result['matches'][2]['metadata']['text']+\"\\n\"+result['matches'][3]['metadata']['text']+\"\\n\"+result['matches'][4]['metadata']['text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFT9-I64HgR9"
   },
   "source": [
    "# Apply Chatgpt TO Get Refined Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OX4ftMv21YZi"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the OpenAI API key\n",
    "openai.api_key = ''\n",
    "\n",
    "def return_answer (context,query):\n",
    "  # Make a request using the OpenAI API\n",
    "  response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": '''Fallow all the Following Instrcutions Given Below\n",
    "        1.You are a helpful AI assistant I am going to Pass You Context and Query inside user content.\n",
    "        2.Provide the Best, Professional, Error Free, Cleaned and Refined Answer according to User Query Based on Context.\n",
    "        3.Revise the provided text to eliminate any spacing errors and ensure a professional presentation.\n",
    "        4.Correct spacing should be applied consistently throughout the text.'''},\n",
    "        {\"role\": \"user\", \"content\": f\"Context: \\n{context}\\n\\nQuery: {query}\"}\n",
    "        ],\n",
    "    temperature=0,\n",
    "    max_tokens=3000,\n",
    "    top_p=0,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0)\n",
    "  return response['choices'][0]['message']['content']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGbj-7VeHqK5"
   },
   "source": [
    "#Calling Get Similarity and Return Answer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "edCuM3MT93MU"
   },
   "outputs": [],
   "source": [
    "def get_answer(query):\n",
    "  context = get_similiar_docs(pinecone_index,query)\n",
    "  answer = return_answer (context,query)\n",
    "  return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3OSBBkgIIem"
   },
   "source": [
    "# Enter Your Query to Get Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IptazEu29_3U",
    "outputId": "1ab45de0-d98b-436b-dca0-dfe16185f412"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_community.embeddings.openai:Warning: model not found. Using cl100k_base encoding.\n"
     ]
    }
   ],
   "source": [
    "Enter_Your_query='Consider postponing the application if'\n",
    "answer = get_answer(Enter_Your_query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FVohEyiIMnI"
   },
   "source": [
    "#Printing Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l4nLZolKK8Rc",
    "outputId": "53d2a670-9e36-4795-eba4-088a23a9f570"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The application should be considered for postponing if the following conditions apply:\n",
      "\n",
      "1. The applicant has not met the standard of medical care as outlined below:\n",
      "   - Ages 60 to 64, must have received doctor’s care within the past 5 years.\n",
      "   - Ages 65 to 70, must have received doctor’s care within the past 2 years.\n",
      "   - Ages 71 to 75, must have received doctor’s care within the past year.\n",
      "   If the application is submitted and formally postponed for not meeting these requirements, the applicant will be required to establish care with a primary care physician with more than one visit and any labs and/or age-appropriate testing completed. Copies of the records should be submitted at no expense to Nationwide at the time of reapplication.\n",
      "\n",
      "2. The applicant is being evaluated for an undiagnosed medical condition. In this case, it is advisable to postpone the submission until all evaluations have been completed and a diagnosis has been made.\n",
      "\n",
      "3. The applicant has any outstanding tests, lab work, follow-ups or referrals pending. The submission should be postponed until all evaluations have been completed.\n",
      "\n",
      "4. The applicant has had a surgery completed, has a surgery scheduled within the next 6 months or has been advised to have surgery. The submission should be postponed until the applicant is at least 3 months postoperative, fully recovered, back to 100% activity and released from all medical and doctor’s care.\n",
      "\n",
      "5. The applicant has undergone spinal or back surgery. Applications should not be submitted prior to 12 months from completion of treatment, including physical therapy.\n",
      "\n",
      "6. The applicant has undergone surgeries and/or injection treatment for joint disorders. Applications should not be submitted if the treatments were completed during the past 6 months. If completed within the past 6 to 12 months or if multiple joints were involved, it should be discussed with the insurance professional. \n",
      "\n",
      "7. The applicant is scheduled for surgery or needs surgery within the next six months. The submission should be postponed until the client is fully recovered, three months post-operation, released from medical care and back to 100 percent activity.\n",
      "\n",
      "8. The applicant is receiving physical therapy. The submission should be postponed until the client is back to 100 percent activity without limitations and care. \n",
      "\n",
      "Please note that these are the minimum care guidelines, and specific medical histories could warrant shorter interval follow-up times, regardless of age.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
